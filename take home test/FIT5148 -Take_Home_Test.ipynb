{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oD5VW0TzHdR_"
   },
   "source": [
    "# FIT5148 - Distributed Databases and Big Data\n",
    "\n",
    "# Take Home Test - Solution Workbook#\n",
    "\n",
    "This test consists of three questions total worth 5% of the final marks. The first question is related to ** Parallel Search Algorithms (1 Marks)**, the second question is related to ** Parallel Join Algorithms (2 Marks)** and the third question is realted to ** Parallel Sort and GroupBy Algorithms (2 Marks)**.\n",
    "\n",
    "**Instructions:**\n",
    "- You will be using Python 3.\n",
    "- Read the instructions, code base and comments carefully.\n",
    "- There are code blocks that **you need to complete** yourself as a part of test.\n",
    "- <font color='red'> **Comment each line of code properly such that the tutor can easily understand what you are trying to do in the code.**</font>\n",
    "\n",
    "**Your Details:**\n",
    "- Name: Shih-Ting Chu\n",
    "- StudentID: 29286875\n",
    "- Email: schu0020@student.monash.edu\n",
    "\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kRWxd1YrHdSB"
   },
   "source": [
    "\n",
    "\n",
    "### Dataset ###\n",
    "For this test, we will use the following two tables R and S to write the solutions to three parallel algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ImhBOqgmHdSC"
   },
   "outputs": [],
   "source": [
    "# R consists of 15 pairs, each comprising two attributes (nominal and numeric)\n",
    "R = [('Adele',8),('Bob',22),('Clement',16),('Dave',23),('Ed',11),\n",
    "     ('Fung',25),('Goel',3),('Harry',17),('Irene',14),('Joanna',2),\n",
    "     ('Kelly',6),('Lim',20),('Meng',1),('Noor',5),('Omar',19)]\n",
    "\n",
    "# S consists of 8 pairs, each comprising two attributes (nominal and numeric)\n",
    "S = [('Arts',8),('Business',15),('CompSc',2),('Dance',12),('Engineering',7),\n",
    "     ('Finance',21),('Geology',10),('Health',11),('IT',18)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7W3uUvJLErBY"
   },
   "source": [
    "### 1. Parallel Searching Algorithm ###\n",
    "In this task, you will build a **parallel search algorithm for range selection (continuous)** for a given query. You will implement one particular search algorithm which is instructed below.\n",
    "\n",
    " **Implement a parallel search algorithm** that uses the linear search algorithm (i.e. **`linear_search()`**) and is able to work with the hash partitioning method (i.e.**` h_partition()`**). \n",
    " **Complete the code block between \"### START CODE HERE ###\" and \"### END CODE HERE ###\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CvTYiOkyFJp8"
   },
   "outputs": [],
   "source": [
    "# linear search function\n",
    "def linear_search(data, key):\n",
    "    \"\"\"\n",
    "    Perform linear search on data for the given key\n",
    "\n",
    "    Arguments:\n",
    "    data -- an input dataset which is a list or a numpy array\n",
    "    key -- an query record\n",
    "\n",
    "    Return:\n",
    "    result -- the position of searched record\n",
    "    \"\"\"\n",
    "    \n",
    "    matched_record = None # initialise as None\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    # value is ('Adele',8); value[0] is Adele; value[1] is 8 #\n",
    "    for value in data: # check each value in data\n",
    "        if value[1] == key: # if value[1] is matched with key\n",
    "            matched_record = value # now the matched record is that value[1]\n",
    "            break # break the loop\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return matched_record # return the matched record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "An_0xFW2FQvs"
   },
   "outputs": [],
   "source": [
    "# define a simple hash function.\n",
    "def s_hash(input_record, p):\n",
    "    \"\"\"\n",
    "    Define a simple hash function for demonstration\n",
    "\n",
    "    Arguments:\n",
    "    input_record -- an input record\n",
    "    p -- the number of processors\n",
    "\n",
    "    Return:\n",
    "    result -- the hash value of input_record\n",
    "    \"\"\"\n",
    "    result = input_record%p # remainder of input_record/p\n",
    " \n",
    "    return result # return the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JMLZCK2BFYeF"
   },
   "outputs": [],
   "source": [
    "# hash data partitionining function\n",
    "# use the \"s_hash\" function defined above to realise this partitioning\n",
    "def h_partition(data, p):\n",
    "    \"\"\"\n",
    "    Perform hash data partitioning on data\n",
    "\n",
    "    Arguments:\n",
    "    data -- an input dataset which is a list\n",
    "    p -- the number of processors\n",
    "\n",
    "    Return:\n",
    "    result -- the paritioned subsets of D\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    partitions = {} # use a dictionary\n",
    "    for input_record in data: # each record in data, perform the following\n",
    "        h = s_hash(input_record[1], p) # get the hash key of the input\n",
    "        if (h in partitions.keys()): # if the key exists\n",
    "            s = partitions[h] # s indicates a set\n",
    "            s.add(input_record) # add input_record into set s\n",
    "            partitions[h] = s # add the new input to the value set of the key\n",
    "        else: # if the key does not exist\n",
    "            s = set() # create an empty value set\n",
    "            s.update({input_record}) # update() adds elements from a set to the set\n",
    "            partitions[h] = s # add the value set to the key\n",
    "            \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return partitions # return the partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xlVKTCO-FkV9"
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "# parallel searching algorithm for range selection\n",
    "def parallel_search_range(data, query_range, n_processor):\n",
    "    \"\"\"\n",
    "    Perform parallel search for range selection on data for the given key\n",
    "\n",
    "    Arguments:\n",
    "    data -- the input dataset which is a list\n",
    "    query_range -- a query record in the form of a range (e.g. [30, 50])\n",
    "    n_processor -- the number of parallel processors\n",
    "    \n",
    "    Return:\n",
    "    results -- the matched record information\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    pool = Pool(processes=n_processor)\n",
    "\n",
    "    ### START CODE HERE ###     \n",
    "    \n",
    "    # perform data partitioning first\n",
    "    DD = h_partition(data, n_processor)\n",
    " \n",
    "\n",
    "    # 要想成分割好處理器後，從資料集中一個個資料去比對（而非從5開始去返回比對處理器中的資料）\n",
    "#     for partition in DD:\n",
    "#         result = pool.apply_async(linear_search, [data, query_range])\n",
    "    \n",
    "#     pool.close()\n",
    "#     pool.join()\n",
    "    \n",
    "#     results = result.get()\n",
    "    \n",
    "    \n",
    "    # get the ID from data\n",
    "    query_list = []\n",
    "    for each in data:\n",
    "        query_list.append(each[1])\n",
    "    \n",
    "    # each element in DD has a pair (hash key: records)\n",
    "    # from the first query_range to the last query_range, step = 1 (check 1 by 1)\n",
    "    for query in range(query_range[0], query_range[1], 1):\n",
    "        query_hash = s_hash(query, n_processor) # check which processor it should be in\n",
    "        d = list(DD[query_hash]) # list all values in the processor which has the same remainder as query_hash\n",
    "        result = pool.apply(linear_search, [d, query]) # apply linear_search\n",
    "        if query in query_list:\n",
    "            results.append(result)\n",
    "            \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return results # return the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vgfN1IcyFxaG",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Noor', 5), ('Kelly', 6), ('Adele', 8), ('Ed', 11), ('Irene', 14), ('Clement', 16), ('Harry', 17), ('Omar', 19)]\n"
     ]
    }
   ],
   "source": [
    "n_processor = 3\n",
    "# range partition, linear_search \n",
    "results = parallel_search_range(R, [5, 20], n_processor)\n",
    "print(results) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qf29MfmUGWV1"
   },
   "source": [
    "## 2. Parallel Join Algorithm\n",
    "\n",
    "In this task, you will implement a **disjoint-partitioning based parallel join algorithm**. This algorithm consist of two stages: a data partitioning stage using a disjoint partitioning and a local join.\n",
    "\n",
    " \n",
    "As a data partitioning method, use the range partitioninig method  (i.e. **`range_partition( )`**).\n",
    "Assume that we have **3 parallel processors**, processor 1 will get records with join attribute value between 1 and 9, processor 2 between 10 and 19, and processor 3 between 20 and 29. Note that both tables R and S need to be partitioned based on the join attribute with the same range partitioning function. \n",
    "\n",
    "As a joining technique, use the hash based join algorithm (i.e.**`HB_join( )`** ).  **Complete the code block between \"### START CODE HERE ###\" and \"### END CODE HERE ###\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y_BwmzrmHaTN"
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "# range data partitionining function\n",
    "def range_partition(data, range_indices):\n",
    "    \"\"\"\n",
    "    Perform range data partitioning on data based on the join attribute\n",
    "\n",
    "    Arguments:\n",
    "    data -- an input dataset which is a list\n",
    "    range_indices -- the index list of ranges to be s:plit\n",
    "\n",
    "    Return:\n",
    "    result -- the paritioned subsets of D\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    \n",
    "    ### START CODE HERE ###  \n",
    "    \n",
    "    # sort the dataset according their values first\n",
    "    new_data = list(data)\n",
    "    new_data.sort(key = operator.itemgetter(1))\n",
    "#     new_data.sort(key = lambda new_data:new_data[1]) ### another way to sort ###\n",
    "\n",
    "\n",
    "    # calculate the number of bins\n",
    "    n_bin = len(range_indices)\n",
    "\n",
    "    # for each bin, perform the following\n",
    "    for i in range(n_bin): \n",
    "        # find elements to be belonging to each range\n",
    "        s = [x for x in new_data if x[1] < range_indices[i]] # s refers a list to store elements in the specific range\n",
    "        # add the partitioned list to the result\n",
    "        result.append(s) \n",
    "        # find the last element in the previous partition\n",
    "        last_element = s[len(s)-1]\n",
    "        # find the index of the last element\n",
    "        last = new_data.index(last_element)\n",
    "        # remove the partitioned list from the dataset\n",
    "        new_data = new_data[int(last)+1:] # a list of the remaining elements\n",
    "\n",
    "        # append the last remaining data list\n",
    "    result.append([x for x in new_data if x[1] >= range_indices[n_bin-1]]) \n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result # return the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7kONrahsIMmD"
   },
   "outputs": [],
   "source": [
    "def H(r):\n",
    "    \"\"\"\n",
    "    We define a hash function 'H' that is used in the hashing process works \n",
    "    by summing the first and second digits of the hashed attribute, which\n",
    "    in this case is the join attribute. \n",
    "    \n",
    "    Arguments:\n",
    "    r -- a record where hashing will be applied on its join attribute\n",
    "\n",
    "    Return:\n",
    "    result -- the hash index of the record r\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert the value of the join attribute into the digits\n",
    "    digits = [int(d) for d in str(r[1])]\n",
    "    \n",
    "    # calulate the sum of elemenets in the digits\n",
    "    return sum(digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gEpFbToJIPlr"
   },
   "outputs": [],
   "source": [
    "def HB_join(T1, T2):\n",
    "    \"\"\"\n",
    "    Perform the hash-based join algorithm.\n",
    "    The join attribute is the numeric attribute in the input tables T1 & T2\n",
    "\n",
    "    Arguments:\n",
    "    T1 & T2 -- Tables to be joined\n",
    "\n",
    "    Return:\n",
    "    result -- the joined table\n",
    "    \"\"\"\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    \n",
    "    dic = {} # use a dictionary\n",
    "    \n",
    "    # for each record in table T2\n",
    "    for s in T2:\n",
    "        # hash the record based on join attribute value using hash function H into hash table\n",
    "        s_key = H(s)\n",
    "        if s_key in dic:\n",
    "            dic[s_key].add(s) # if there is an entry\n",
    "        else:\n",
    "            dic[s_key] = {s}\n",
    "            \n",
    "    # for each record in table T1 (probing)\n",
    "    for r in T1:\n",
    "        # hash the record based on join attribute value using H\n",
    "        r_key = H(r)\n",
    "\n",
    "        # if an index entry is found, then do the following\n",
    "        if r_key in dic:\n",
    "            # compare each record on this index entry with the record of table T1\n",
    "            for item in dic[r_key]:\n",
    "                if item[1] == r[1]:\n",
    "                    # append the result\n",
    "                    result.append({\", \".join([r[0], str(r[1]), item[0]])})\n",
    "    \n",
    "    return result # return the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9cpQYKvvH241"
   },
   "outputs": [],
   "source": [
    "# include this package for parallel processing\n",
    "import multiprocessing as mp\n",
    "\n",
    "def DPBP_join(T1, T2, n_processor):\n",
    "    \"\"\"\n",
    "    Perform a disjoint partitioning-based parallel join algorithm.\n",
    "    The join attribute is the numeric attribute in the input tables T1 & T2\n",
    "\n",
    "    Arguments:\n",
    "    T1 & T2 -- Tables to be joined\n",
    "    n_processor -- the number of parallel processors\n",
    "\n",
    "    Return:\n",
    "    result -- the joined table\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    # partition T1 & T2 into sub-tables using range_partition()\n",
    "    # the number of the sub-tables must be the equal to the n_processor\n",
    "    T1_subsets = range_partition(T1, [10, 20])\n",
    "    T2_subsets = range_partition(T2, [10, 20])\n",
    "    \n",
    "    # pool: a Python method enabling parallel processing \n",
    "    pool = mp.Pool(processes = n_processor)\n",
    "    \n",
    "    midResults = []\n",
    "    for i in range(len(T1_subsets)):\n",
    "        # apply a join on each processor\n",
    "        output = pool.apply_async(HB_join, [T1_subsets[i], T2_subsets[i]])\n",
    "       \n",
    "        midResults.append(output)\n",
    "        \n",
    "        \n",
    "    for result in midResults:\n",
    "        results.append(result.get())\n",
    "        \n",
    "        #results.append(pool.apply(HB_join, [T1_subsets[i], T2_subsets[i]]))\n",
    "        \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lJgTe8pVH_0z"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'Joanna, 2, CompSc'}, {'Adele, 8, Arts'}], [{'Ed, 11, Health'}], []]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_processor = 3\n",
    "DPBP_join(R, S, n_processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W9vIqdbjJaXv"
   },
   "source": [
    "## 3. Parallel Sorting Algorithm\n",
    "\n",
    "In this task, you will implement **parallel binary-merge sort** method. It has two phases same as the parallel merge-all sort that you learnt in the labs: (1) local sort and (2) final merge. The first phase is similar to the parallel merge-all sort. The second phase, the merging phase, is pipelined instead of concentrating on one processor. In this phase, we take the results from two processors and then merging the two in one processor, called binary merging. The result of the merging between two processors is passed on to the next level until one processor (the host) is left.\n",
    "\n",
    " **Complete the code block between \"### START CODE HERE ###\" and \"### END CODE HERE ###\".**\n",
    "Assume that we use the round robin partitioning method  (i.e. **`rr_partition()`**). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gd16AZF_LgWp"
   },
   "outputs": [],
   "source": [
    "def qsort(data): \n",
    "\n",
    "    \"\"\" \n",
    "    Quicksort a list\n",
    "    \n",
    "    Arguments:\n",
    "    arr -- the input list to be sorted\n",
    "\n",
    "    Return:\n",
    "    result -- the sorted arr\n",
    "    \"\"\"\n",
    "    if len(data) <= 1: # if there is only 1 or less than 1 data record\n",
    "        return data # return the data\n",
    "    else:\n",
    "        pivot = data[0][1] # take the value of first element as the pivot\n",
    "        left_data = [x for x in data[1:] if x[1] < pivot] # compare the element's value to pivot's\n",
    "        right_data = [x for x in data[1:] if x[1] >= pivot] # compare the element's value to pivot's\n",
    "        value_list = qsort(left_data) + [data[0]] + qsort(right_data) # combine the lists to 1 list\n",
    "        \n",
    "        return value_list # return a list sorted by the value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# round-robin data partitionining function\n",
    "def rr_partition(data, n):\n",
    "    \"\"\"\n",
    "    Perform data partitioning on data\n",
    "\n",
    "    Arguments:\n",
    "    data -- an input dataset which is a list\n",
    "    n -- the number of processors\n",
    "\n",
    "    Return:\n",
    "    result -- the paritioned subsets of D\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for i in range(n):\n",
    "        result.append([])\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    # calculate the number of the elements to be allocated to each bin\n",
    "    n_bin = len(data)/n\n",
    "    \n",
    "    # for each bin, perform the following\n",
    "    for index, element in enumerate(data): \n",
    "        # calculate the index of the bin that the current data point will be assigned\n",
    "        index_bin = (int) (index % n)\n",
    "        result[index_bin].append(element)\n",
    "        \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result # return the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m3vxcrs-LVaG"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# find the smallest record\n",
    "def find_min(records):    \n",
    "    \"\"\" \n",
    "    Find the smallest record\n",
    "    \n",
    "    Arguments:\n",
    "    records -- the input record set\n",
    "\n",
    "    Return:\n",
    "    result -- the smallest record's index\n",
    "    \"\"\"\n",
    "    m = records[0] # the first record data\n",
    "    index = 0 # the first index\n",
    "    for i in range(len(records)): # for each record\n",
    "        if(records[i][1] < m[1]):  # if the current record's value < m's\n",
    "            index = i # point to that index of this record\n",
    "            m = records[i] # now m changed\n",
    "    return index # return the index of that smallest record\n",
    "\n",
    "\n",
    "def k_way_merge(record_sets):\n",
    "    \"\"\" \n",
    "    K-way merging algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    record_sets -- the set of mulitple sorted sub-record sets\n",
    "\n",
    "    Return:\n",
    "    result -- the sorted and merged record set\n",
    "    \"\"\"\n",
    "    \n",
    "    # indexes will keep the indexes of sorted records in the given buffers\n",
    "    indexes = []\n",
    "    for x in record_sets:\n",
    "        indexes.append(0) # initialisation with 0\n",
    "\n",
    "    # final result will be stored in this variable\n",
    "    result = []  \n",
    "    \n",
    "    while(True):\n",
    "        merged_result = [] # the merging unit\n",
    "        \n",
    "        # this loop gets the current position of every buffer\n",
    "        for i in range(len(record_sets)):\n",
    "            if(indexes[i] >= len(record_sets[i])): # check if go through all values in this subfile\n",
    "                merged_result.append(('sys max', sys.maxsize)) # if yes, add a largest positive integer to merged_result\n",
    "            else:\n",
    "                merged_result.append(record_sets[i][indexes[i]]) # if not finished yet, add the record\n",
    "        \n",
    "        # find the smallest record \n",
    "        smallest = find_min(merged_result) # smallest refers to the index of that record\n",
    "    \n",
    "        # if we only have sys.maxsize on the tuple, we reached the end of every record set\n",
    "        if(merged_result[smallest][1] == sys.maxsize):\n",
    "            break # break the loop\n",
    "\n",
    "        # this record is the next on the merged list\n",
    "        result.append(record_sets[smallest][indexes[smallest]])\n",
    "        indexes[smallest] +=1\n",
    "   \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Adele', 1), ('Fung', 2), ('Ed', 3), ('W', 4), ('Goel', 5), ('Bob', 6), ('Clement', 7), ('Harry', 8), ('RR', 9)]\n"
     ]
    }
   ],
   "source": [
    "# Test k-way merging method\n",
    "buffers = [[('Adele',1),('Bob',6),('Clement',7)], [('Fung',2),('Goel',5),('Harry',8)], [('Ed', 3), ('W', 4), ('RR', 9)]]\n",
    "# print(buffers[0][1][1])\n",
    "result = k_way_merge(buffers)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yof8Q84YLcOU"
   },
   "outputs": [],
   "source": [
    "def serial_sorting(dataset, buffer_size):\n",
    "    \"\"\"\n",
    "    Perform a serial external sorting method based on sort-merge\n",
    "    The buffer size determines the size of eac sub-record set\n",
    "\n",
    "    Arguments:\n",
    "    dataset -- the entire record set to be sorted\n",
    "    buffer_size -- the buffer size determining the size of each sub-record set\n",
    "\n",
    "    Return:\n",
    "    result -- the sorted record set\n",
    "    \"\"\"\n",
    "    \n",
    "    if (buffer_size <= 2):\n",
    "        print(\"Error: buffer size should be greater than 2\")\n",
    "        return\n",
    "    \n",
    "    result = []\n",
    "\n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    # --- Sort Phase ---\n",
    "    sorted_set = []\n",
    "    \n",
    "    # read buffer_size pages at a time into memory and\n",
    "    # sort them, and write out a sub-record set (i.e. variable: subset)\n",
    "    start_pos = 0\n",
    "    N = len(dataset)\n",
    "    while True:\n",
    "        if ((N - start_pos) > buffer_size):\n",
    "            # read B-records from the input, where B = buffer_size\n",
    "            subset = dataset[start_pos:start_pos + buffer_size] \n",
    "            # sort the subset (using qucksort defined above) and add into sorted_set\n",
    "            sorted_subset = qsort(subset) \n",
    "            sorted_set.append(sorted_subset)\n",
    "            start_pos += buffer_size\n",
    "        else:\n",
    "            # read the last B-records from the input, where B is less than buffer_size\n",
    "            subset = dataset[start_pos:] \n",
    "            # sort the subset (using qucksort defined above)\n",
    "            sorted_subset = qsort(subset) \n",
    "            sorted_set.append(sorted_subset)\n",
    "            break\n",
    "    \n",
    "    # --- Merge Phase ---\n",
    "    merge_buffer_size = buffer_size - 1 # use B-1 buffers for input and 1 buffer for output\n",
    "    dataset = sorted_set\n",
    "    while True:\n",
    "        merged_set = []\n",
    "\n",
    "        N = len(dataset)\n",
    "        start_pos = 0\n",
    "        while True:\n",
    "            if ((N - start_pos) > merge_buffer_size): \n",
    "                # read C-record sets from the merged record sets, where C = merge_buffer_size\n",
    "                subset = dataset[start_pos:start_pos + merge_buffer_size]\n",
    "                merged_set.append(k_way_merge(subset)) # merge lists in subset\n",
    "                start_pos += merge_buffer_size\n",
    "            else:\n",
    "                # read C-record sets from the merged sets, where C is less than merge_buffer_size\n",
    "                subset = dataset[start_pos:]\n",
    "                merged_set.append(k_way_merge(subset)) # merge lists in subset\n",
    "                break\n",
    "\n",
    "        dataset = merged_set\n",
    "        if (len(dataset) <= 1): # if the size of merged record set is 1, then stop \n",
    "            result = merged_set\n",
    "            break # break the loop\n",
    "            \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b_jH8jXwLKRT"
   },
   "outputs": [],
   "source": [
    "# Include this package for parallel processing\n",
    "import multiprocessing as mp\n",
    "\n",
    "def parallel_binary_merge_sorting(dataset, n_processor, buffer_size):\n",
    "    \"\"\"\n",
    "    Perform a parallel binary-merge sorting method\n",
    "\n",
    "    Arguments:\n",
    "    dataset -- entire record set to be sorted\n",
    "    n_processor -- number of parallel processors\n",
    "    buffer_size -- buffer size determining the size of each sub-record set\n",
    "\n",
    "    Return:\n",
    "    result -- the merged record set\n",
    "    \"\"\"\n",
    "    \n",
    "    if (buffer_size <= 2):\n",
    "        print(\"Error: buffer size should be greater than 2\")\n",
    "        return\n",
    "    \n",
    "    result = []\n",
    "\n",
    "    ### START CODE HERE ### \n",
    "    # Pre-requisite: Perform data partitioning using round-robin partitioning\n",
    "    subsets = rr_partition(dataset, n_processor)\n",
    "    \n",
    "    # Pool: a Python method enabling parallel processing. \n",
    "    pool = mp.Pool(processes = n_processor)\n",
    "\n",
    "    # ----- Sort phase -----\n",
    "    sorted_set = []\n",
    "    for s in subsets:\n",
    "        # call the serial_sorting method above\n",
    "        sorted_set.append(*pool.apply_async(serial_sorting, [s, buffer_size]).get())\n",
    "    pool.close()\n",
    "    \n",
    "    # ---- Final merge phase ----\n",
    "    print(\"Sorted entire set:\\n\" + str(sorted_set))\n",
    "    dataset = sorted_set\n",
    "    while True:\n",
    "        merged_set = []\n",
    "\n",
    "        N = len(dataset)\n",
    "        start_pos = 0\n",
    "        pool = mp.Pool(processes = N//2)\n",
    "\n",
    "        while True:\n",
    "            if ((N - start_pos) > 2): \n",
    "                subset = dataset[start_pos:start_pos + 2]\n",
    "                merged_set.append(pool.apply(k_way_merge, [subset]))\n",
    "                start_pos += 2\n",
    "            else:\n",
    "                subset = dataset[start_pos:]\n",
    "                merged_set.append(pool.apply(k_way_merge, [subset]))\n",
    "                break\n",
    "        \n",
    "        pool.close()\n",
    "        dataset = merged_set\n",
    "        \n",
    "        if (len(dataset) == 1): # if the size of merged record set is 1, then stop \n",
    "            result = merged_set\n",
    "            break\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test round robin partition\n",
    "# subset = rr_partition(R, 10)\n",
    "# subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aMu19MwXLxNd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted entire set:\n",
      "[[('Kelly', 6), ('Adele', 8)], [('Lim', 20), ('Bob', 22)], [('Meng', 1), ('Clement', 16)], [('Noor', 5), ('Dave', 23)], [('Ed', 11), ('Omar', 19)], [('Fung', 25)], [('Goel', 3)], [('Harry', 17)], [('Irene', 14)], [('Joanna', 2)]]\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "Final Result:\n",
      "[[('Meng', 1), ('Joanna', 2), ('Goel', 3), ('Noor', 5), ('Kelly', 6), ('Adele', 8), ('Ed', 11), ('Irene', 14), ('Clement', 16), ('Harry', 17), ('Omar', 19), ('Lim', 20), ('Bob', 22), ('Dave', 23), ('Fung', 25)]]\n"
     ]
    }
   ],
   "source": [
    "result = parallel_binary_merge_sorting(R, 10, 20)\n",
    "print(\"\\n\" + \"-\"*90 + \"\\n\")\n",
    "print(\"Final Result:\\n\" + str(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FIT5148 -Take_Home_Test.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
